#summary High performance GPU radix sorting in CUDA

<table><tr><td>
http://back40computing.googlecode.com/svn/trunk/images/SortingSmall.jpg
</td><td valign="top">
<wiki:toc max_depth="1" />
</td></tr></table>

<BR><BR>
----

=Sorting Overview=

We have designed extremely efficient strategies for sorting large sequences of fixed-length keys (and values) using GPU stream processors.  Specifically, this project implements a very fast, efficient radix sorting method for CUDA-capable devices (e.g., NVIDIA GPUs).  Our critical performance insights leverage:
 * Very efficient, generalized parallel prefix scan
 * Kernel fusion

Our radix sorting methods exhibit multiple factors of speedup over state-of-the-art
 * GPU sorting implementations (e.g., [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5161005 CUDPP])  
 * CPU implementations (e.g., [http://doi.acm.org/10.1145/1807167.1807207 for the Intel quad-core Core i7])
 * Experimental many-core implementations (e.g., [http://doi.acm.org/10.1145/1454159.1454171 for the Intel Larrabee] and [http://doi.acm.org/10.1145/1807167.1807207 for the Intel MIC/Knight's Ferry])

In particular, our implementation running on a (stock) NVIDIA GTX 480 is capable of *GigaKey/s sorting rates* (i.e., sorting more than a billion 32-bit keys per second).  


====<FONT color=#006666>Radix Sorting</FONT>====

The radix sorting method works by iterating over the _d_-bit digit-places of the keys from least-significant to most-significant.  For each digit-place, the method performs a stable distribution sort of the keys based upon their digit at that digit-place.  Given an _n_-element sequence of _k_-bit keys and a radix _r_ = 2_^d^_, a radix sort of these keys will require _k_/_d_ iterations of a distribution sort over all n keys.

The distribution sort (a.k.a. counting sort) is the fundamental component of the radix sorting method.  In a data-parallel, shared-memory model of computation, each logical processor gathers its key, decodes the specific digit at the given digit-place, and then must cooperate with other processors to determine where the key should be relocated.  The relocation offset will be the key’s global rank, i.e., the number of keys with “lower” digits at that digit place plus the number of keys having the same digit, yet occurring earlier in the input sequence.  

We implement these distribution sorting passes using a very efficient implementation of a generalized parallel prefix scan.  Our generalized scan is designed to operate over multiple, concurrent scan problems.  For example, with _d_ = 4 bits (_r_ = 16 digits), our multi-scan does sixteen scan operations: one for each digit.  For each of the scan operations (e.g., the 0s scan, the 1s scan, the 2s scan, etc.), the input for each key is a 1 if the key's digit place contains that operation's digit, 0 otherwise.  When the mulit-scan is done, the logical processor for each key can look up the scan result from the appropriate scan operation to determine where its key should be placed.

====<FONT color=#006666>Authors' Request</FONT>====

If you use/reference/benchmark this code, please cite our [http://www.cs.virginia.edu/~dgm4d/papers/RadixSortTR.pdf Technical Report]:

 _Duane Merrill and Andrew Grimshaw, "Revisiting Sorting for GPGPU Stream Architectures," University of Virginia, Department of Computer Science, Charlottesville, VA, USA, Technical Report CS2010-03, 2010._

Bibtex:
{{{
 @TechReport{ Merrill:Sorting:2010,
	author = "Duane Merrill and Andrew Grimshaw",
	title = "Revisiting Sorting for GPGPU Stream Architectures",
	year = "2010",
	institution = "University of Virginia, Department of Computer Science",
	address = "Charlottesville, VA, USA",
	number = "CS2010-03"
 }
}}}

<BR><BR>
----

=Performance=

The following tables present average sorting rates (for saturating problem sizes > 32M elements) on various CUDA GPUs.  These results were measured using a suite of ~2,500 randomly-sized input sequences (sized 32 - 180M elements), each initialized with keys and values whose bits were sampled from a uniformly random distribution.  Our measurements for elapsed time were taken directly from GPU hardware performance counters.  Therefore these results are reflective of in situ sorting problems: they preclude the driver overhead and the overheads of staging data to/from the accelerator, allowing us to directly contrast the individual and cumulative performance of the stream kernels involved.

Please note that the following measurements were made using the Cuda 3.0 compiler and driver framework.  (In our evaluation, code generated by the 3.0 compiler is ~0.5% faster than that generated by the 3.1 compiler.)  For Fermi-class devices, we compiled our device kernels for 32-bit device pointers.  (See the section on [#Building_and_Adapting building and adapting] below).


  ====<FONT color=#006666>8-bit (uchar) Keys (10^6^ elements/s)</FONT>====

|| *Satellite Values*     || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _None (keys-only)_     || ||                  ||               ||         || || 4,310.29  ||                 ||           ||           || || 310.07      ||           ||
|| _32-bit values_        || ||                  ||               ||         || || 3,391.34  ||                 ||           ||           || || 273.62      ||           ||
|| _64-bit values         || ||                  ||               ||         || || 2,416.20  ||                 ||           ||           || || 254.46      ||           ||
|| _128-bit values_       || ||                  ||               ||         || || 888.38    ||                 ||           ||           || || 154.54      ||           ||


  ====<FONT color=#006666>16-bit (ushort) Keys (10^6^ elements/s)</FONT>====

|| *Satellite Values*     || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _None (keys-only)_     || ||                  ||               ||         || || 2,129.14  ||                 ||           ||           || || 184.93      ||           ||
|| _32-bit values_        || ||                  ||               ||         || || 1,656.37  ||                 ||           ||           || || 162.88      ||           ||
|| _64-bit values         || ||                  ||               ||         || || 1,139.69  ||                 ||           ||           || || 150.68      ||           ||
|| _128-bit values_       || ||                  ||               ||         || || 436.78    ||                 ||           ||           || || 81.59       ||           ||


  ====<FONT color=#006666>32-bit (uint) Keys (10^6^ elements/s)</FONT>====

|| *Satellite Values*     || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _None (keys-only)_     || ||                  ||               ||         || || 1,005.49  ||                 ||           ||           || || 265.19      ||           ||
|| _32-bit values_        || ||                  ||               ||         || || 775.12    ||                 ||           ||           || || 188.71      ||           ||
|| _64-bit values         || ||                  ||               ||         || || 492.28    ||                 ||           ||           || || 169.73      ||           ||
|| _128-bit values_       || ||                  ||               ||         || || 206.16    ||                 ||           ||           || || 54.86       ||           ||

  
  ====<FONT color=#006666>64-bit (ulong) Keys (10^6^ elements/s)</FONT>====

|| *Input Data*           || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _8-bit keys (uchar)_   || ||                  ||               ||         || || 304.86    ||                 ||           ||           || || 70.21       ||           ||
|| _16-bit keys (ushort)_ || ||                  ||               ||         || || 224.30    ||                 ||           ||           || || 54.86       ||           ||
|| _32-bit keys (uint)_   || ||                  ||               ||         || || 186.40    ||                 ||           ||           || || 51.20       ||           ||
|| _64-bit keys (ulong)_  || ||                  ||               ||         || || 90.61     ||                 ||           ||           || || 24.23       ||           ||

† 16M+ elements (restricted by global memory size)<BR>
†† 8M+ elements (restricted by global memory size)

Of course, our implementation also allows keys and values may be of different types (e.g., `ushort` and `double`).


====<FONT color=#006666>Performance Plots</FONT>====

Below are plots of sorting rate as a function of problem size for 32-bit (uint) keys:

http://back40computing.googlecode.com/svn/trunk/images/sorting/32bitkeys.PNG 

And 32-bit (uint) keys and values:

http://back40computing.googlecode.com/svn/trunk/images/sorting/32bitpairs.PNG



<BR><BR>
----

=Building and Adapting=

Like [http://code.google.com/p/thrust/ Thrust], the sources for our implementation are designed to be included and built as part of larger applications (as opposed to linked in as a shared or static library).  This prevents kernel bloat and allows the implementation to tailor itself to your data types. The current implementation can sort:
 * Keys of any C/C++ built-in, numeric type (e.g., `signed char`, `float`, `unsigned long long`, etc.)
 * Any structured payload type (within reason).

You can obtain the source code for SRTS Radix Sort (and other B40C subprojects from [http://code.google.com/p/back40computing/source/checkout the SVN repository].  This subproject can be found in the `/trunk/SrtsRadixSort` subtree.  It contains C++/CUDA code for:

 * The GPU kernel sources
 * A templated API for invoking a sorting operation from a host program.  By including the sorting API like a header file, `nvcc` will specialize kernels and runtime code needed to sort the data types specified by the enclosing application.
 * A simple toy program (and a more advanced benchtest program) that demonstrate how the sorting implementation can be integrated into and called from host programs.

The subproject sources require compilation using CUDA Toolkit version 3.0 or higher.  The repository includes a `Makefile` to build the example programs on Linux.  This Makefile can be used as a guide for constructing an equivalent VC++ project for evaluation on Windows systems.  In addition, we suggest that you compile the kernels with the following options:

 * 32-bit device pointers, regardless of whether we're on a 64-bit machine or not.  64-bit device pointers incur ~10-15% slowdown because it causes fairly increased kernel register counts which prevents us from being able to meet our targeted occupancies.
 * Without the newly-introduced-for-3.1 ABI (that gives programmers a program stack).  Even though this code does not utilize a program stack for sorting, the ABI incurs extra register pressure regardless and results in a ~1-5% slowdown.   



<BR><BR>
----

Cheers!