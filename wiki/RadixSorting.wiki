#summary High performance GPU radix sorting in CUDA

<table><tr><td>
http://back40computing.googlecode.com/svn/trunk/images/SortingSmall.jpg
</td><td valign="top">
<wiki:toc max_depth="1" />
</td></tr></table>

<BR><BR>
----

=GigaKey'/'s Sorting Overview=

We have designed extremely efficient strategies for sorting large sequences of fixed-length keys (and values) using GPU stream processors.  Specifically, this project implements a very fast, efficient radix sorting method for CUDA-capable devices (e.g., NVIDIA GPUs).  Our critical performance insights leverage:
 * Very efficient, generalized parallel prefix scan
 * Kernel fusion

Our radix sorting methods exhibit 2.0-3.7x speedup over state-of-the-art GPU implementations (e.g., [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5161005 CUDPP's implementation]) For this domain of sorting problems, we believe our sorting primitive to be the fastest available for any fully-programmable microarchitecture, including [http://doi.acm.org/10.1145/1454159.1454171 implementations for the Intel Larrabee] and [http://doi.acm.org/10.1145/1807167.1807207 for the Intel MIC/Knight's Ferry] accelerators.  

In particular, our implementation running on a (stock) NVIDIA GTX 480 is capable of GigaKey/s sorting rates (i.e., sorting more than a billion 32-bit keys per second).  


====<FONT color=#003333>Radix Sorting</FONT>====

The radix sorting method works by iterating over the _d_-bit digit-places of the keys from least-significant to most-significant.  For each digit-place, the method performs a stable distribution sort of the keys based upon their digit at that digit-place.  Given an _n_-element sequence of _k_-bit keys and a radix _r_ = 2_^d^_, a radix sort of these keys will require _k_/_d_ iterations of a distribution sort over all n keys.

The distribution sort (a.k.a. counting sort) is the fundamental component of the radix sorting method.  In a data-parallel, shared-memory model of computation, each logical processor gathers its key, decodes the specific digit at the given digit-place, and then must cooperate with other processors to determine where the key should be relocated.  The relocation offset will be the key’s global rank, i.e., the number of keys with “lower” digits at that digit place plus the number of keys having the same digit, yet occurring earlier in the input sequence.  

We implement these distribution sorting passes using a very efficient implementation of a generalized parallel prefix scan.  Our generalized scan is designed to operate over multiple, concurrent scan problems.  For example, with _d_ = 4 bits (_r_ = 16 digits), our multi-scan does sixteen scan operations: one for each digit.  For each of the scan operations (e.g., the 0s scan, the 1s scan, the 2s scan, etc.), the input for each key is a 1 if the key's digit place contains that operation's digit.  When the mulit-scan is done, the logical processor for each key can look up the scan result from the appropriate scan operation to determine where its key should be placed.

====<FONT color=#003333>Authors' Request</FONT>====

If you use/reference/benchmark this code, please cite our [http://www.cs.virginia.edu/~dgm4d/papers/RadixSortTR.pdf Technical Report]:

 _Duane Merrill and Andrew Grimshaw, "Revisiting Sorting for GPGPU Stream Architectures," University of Virginia, Department of Computer Science, Charlottesville, VA, USA, Technical Report CS2010-03, 2010._

Bibtex:
{{{
 @TechReport{ Merrill:Sorting:2010,
	author = "Duane Merrill and Andrew Grimshaw",
	title = "Revisiting Sorting for GPGPU Stream Architectures",
	year = "2010",
	institution = "University of Virginia, Department of Computer Science",
	address = "Charlottesville, VA, USA",
	number = "CS2010-03"
 }
}}}

<BR><BR>
----

=Performance=

The following tables present average sorting rates (for saturating problem sizes > 32M elements) on various CUDA GPUs.  These results were measured using a suite of ~2,500 randomly-sized input sequences (sized 32 - 180M elements), each initialized with keys and values whose bits were sampled from a uniformly random distribution.  Our measurements for elapsed time were taken directly from GPU hardware performance counters.  Therefore these results are reflective of in situ sorting problems: they preclude the driver overhead and the overheads of staging data to/from the accelerator, allowing us to directly contrast the individual and cumulative performance of the stream kernels involved.

Please note that the following measurements were made using the Cuda 3.0 compiler and driver framework.  For Fermi-class devices, we compiled our device kernels for 32-bit device pointer.  (See the section on [#Building_and_Adapting building and adapting] below).

====<FONT color=#003333>Keys Only (10^6^ keys/s)</FONT>====

|| *Input Data*            || *Tesla C2050* || *Tesla C1060* || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || *9800 GTX+* || *8800 GTX ||
|| _8-bit keys (uchar)_    ||               ||               ||           ||                 ||           ||           ||             ||           ||
|| _16-bit keys (ushort)_  ||               ||               ||           ||                 ||           ||           ||             ||           ||
|| _32-bit keys (uint)_    ||               ||               || 1007.39   ||                 || 617.47    || 536.25    || 265.18      ||           ||
|| _64-bit keys (ulong)_   ||               ||               ||           ||                 ||           ||           ||             ||           ||

====<FONT color=#003333>Key-Value Pairs (10^6^ pairs/s)</FONT>====

|| *Input Data*            || *Tesla C2050* || *Tesla C1060* || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || *9800 GTX+* || *8800 GTX ||
|| _8-bit pairs (uchar)_   ||               ||               ||           ||                 ||           ||           ||             ||           ||
|| _16-bit pairs (ushort)_ ||               ||               ||           ||                 ||           ||           ||             ||           ||
|| _32-bit pairs (uint)_   ||               ||               || 776.66    ||                 || 490.14    || 450.30    || 188.84†     ||           ||
|| _64-bit pairs (ulong)_  ||               ||               ||           ||                 ||           ||           ||             ||           ||

† 16M+ elements (restricted by global memory size)<BR>
†† 8M+ elements (restricted by global memory size)


====<FONT color=#003333>Performance Plots</FONT>====

Below are plots of sorting rate as a function of problem size for 32-bit (uint) keys, and 32-bit (uint) keys and values.

http://back40computing.googlecode.com/svn/trunk/images/sorting/32bitkeys.PNG http://back40computing.googlecode.com/svn/trunk/images/sorting/32bitpairs.PNG



<BR><BR>
----

=Building and Adapting=

You can obtain the source code for SRTS Radix Sort (and other B40C subprojects from [http://code.google.com/p/back40computing/source/checkout the SVN repository].  This subproject can be found in the `/trunk/SrtsRadixSort` subtree.  It contains C++/CUDA code for:

 * The GPU kernel sources
 * A templated API for invoking a sorting operation from a host program.  By including the sorting API like a header file, `nvcc` will only generate the appropriate 
 * A simple toy program (and a more advanced benchtest program) that demonstrate how the sorting implementation can be integrated into and called from host programs.