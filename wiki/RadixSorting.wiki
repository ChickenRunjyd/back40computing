#summary High performance GPU radix sorting in CUDA

<table><tr><td>
http://back40computing.googlecode.com/svn/trunk/images/SortingSmall.jpg
</td><td valign="top">
*
  <h1>Table of Contents</h1>
*

 <wiki:toc max_depth="1" />
</td></tr></table>

<BR>
----

=Sorting Overview=

This project implements a very fast, efficient radix sorting method for CUDA-capable devices.  For sorting large sequences of fixed-length keys (and values), we believe our sorting primitive to be the fastest available for any fully-programmable microarchitecture: our stock NVIDIA GTX480 sorting results _*exceed the Giga-keys/sec average sorting rate*_ (i.e., one billion 32-bit keys sorted per second).  

In addition, one of our design goals for this project is flexibility.  We've designed our implementation to adapt itself and perform well on all generations and configurations of programmable NVIDIA GPUs, and for a wide variety of input types.

[#Building_and_Adapting Download] and play around with the implementation yourself!


====<FONT color=#006666>Radix Sorting</FONT>====

The radix sorting method works by iterating over the _d_-bit digit-places of the keys from least-significant to most-significant.  For each digit-place, the method performs a stable distribution sort of the keys based upon their digit at that digit-place.  Given an _n_-element sequence of _k_-bit keys and a radix _r_ = 2_^d^_, a radix sort of these keys will require _k_/_d_ iterations of a distribution sort over all n keys.

The distribution sort (a.k.a. counting sort) is the fundamental component of the radix sorting method.  In a data-parallel, shared-memory model of computation, each logical processor gathers its key, decodes the specific digit at the given digit-place, and then must cooperate with other processors to determine where the key should be relocated.  The relocation offset will be the key’s global rank, i.e., the number of keys with “lower” digits at that digit place plus the number of keys having the same digit, yet occurring earlier in the input sequence.  

We implement these distribution sorting passes using a very efficient implementation of a generalized parallel prefix scan.  Our generalized scan is designed to operate over multiple, concurrent scan problems.  For example, with _d_ = 4 bits (_r_ = 16 digits), our multi-scan does sixteen scan operations: one for each digit.  For each of the scan operations (e.g., the 0s scan, the 1s scan, the 2s scan, etc.), the input for each key is a 1 if the key's digit place contains that operation's digit, 0 otherwise.  When the mulit-scan is done, the logical processor for each key can look up the scan result from the appropriate scan operation to determine where its key should be placed.

====<FONT color=#006666>Authors' Request</FONT>====

If you use/reference/benchmark this code, please cite our [http://www.cs.virginia.edu/~dgm4d/papers/RadixSortTR.pdf Technical Report]:

 _Duane Merrill and Andrew Grimshaw, "Revisiting Sorting for GPGPU Stream Architectures," University of Virginia, Department of Computer Science, Charlottesville, VA, USA, Technical Report CS2010-03, 2010._

Bibtex:
{{{
 @TechReport{ Merrill:Sorting:2010,
	author = "Duane Merrill and Andrew Grimshaw",
	title = "Revisiting Sorting for GPGPU Stream Architectures",
	year = "2010",
	institution = "University of Virginia, Department of Computer Science",
	address = "Charlottesville, VA, USA",
	number = "CS2010-03"
 }
}}}

<BR><BR>
----

=Performance=

The following figures and tables present sorting rates for various CUDA GPUs.  These results were measured using a suite of ~3,000 randomly-sized input sequences (sized 32 - 272M elements), each initialized with keys and values whose bits were sampled from a uniformly random distribution.  Our measurements for elapsed time were taken directly from GPU hardware performance counters.  Therefore these results are reflective of in situ sorting problems: they preclude the driver overhead and the overheads of staging data to/from the accelerator, allowing us to directly contrast the individual and cumulative performance of the stream kernels involved.

Please note that the following measurements were made using the Cuda 3.0 compiler and driver framework.  (In our evaluation, code generated by the 3.0 compiler is ~0.5% faster than that generated by the 3.1 compiler.)  For Fermi-class devices, we compiled our device kernels for 32-bit device pointers.  (See the section on [#Building_and_Adapting building and adapting] below).

We invite the reader to compare our results with the current state-of-the-art for GPUs and CPUs (and for experimental architectures), e.g.:
 * GPU sorting implementations (e.g., [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5161005 CUDPP])  
 * CPU implementations (e.g., [http://doi.acm.org/10.1145/1807167.1807207 for the Intel quad-core Core i7])
 * Experimental many-core implementations (e.g., [http://doi.acm.org/10.1145/1454159.1454171 for the Intel Larrabee] and [http://techresearch.intel.com/userfiles/en-us/FASTsort_CPUsGPUs_IntelMICarchitectures.pdf for the Intel MIC/Knight's Ferry])


===<FONT color=#006666>Selected Performance Plots</FONT>===


  Sorting rates as a function of problem size for 32-bit `uint` keys:

  http://back40computing.googlecode.com/svn/trunk/images/sorting/32bitkeys.PNG 

  <BR>

  And for 32-bit `uint` keys paired with 32-bit values:

  http://back40computing.googlecode.com/svn/trunk/images/sorting/32bitpairs.PNG


===<FONT color=#006666>Average Saturated Sorting Rates</FONT>===

Unless otherwise noted, the following tables present average saturated sorting rates for problem sizes greater than or equal to 32M elements.  Our implementation is also capable of sorting signed and floating-point key types (e.g., `float`, `int`, `double`, etc.) at a  0.5% - 1.5% performance overhead.

*
 Table 1:* 8-bit `uchar` Keys (reported in 10^6^ elements/s)
||                        || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| *Keys-only*            || || 3,188.67         || 2,599.39      ||         || || 4,310.29  ||                 || 2,108.10  ||           || || 310.07      ||           ||
|| *32-bit values*        || || 2,465.85         || 1,774.62      ||         || || 3,391.34  ||                 || 1,852.49  ||           || || 273.62      ||           ||
|| *64-bit values*        || || 1,904.50         || 1,136.93      ||         || || 2,416.20  ||                 || 1,526.05  ||           || || 254.46†     ||           ||
|| *128-bit values*       || || 731.30           || 613.10        ||         || || 888.38    ||                 || 418.77    ||           || || 154.54††    ||           ||


  ====<FONT color=#006666>16-bit `ushort` Keys (10^6^ elements/s)</FONT>====

|| *Satellite Values*     || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _None (keys-only)_     || || 1,572.03         || 1,270.92      ||         || || 2,129.14  ||                 ||           ||           || || 184.93      ||           ||
|| _32-bit values_        || || 1,209.04         || 811.15        ||         || || 1,656.37  ||                 ||           ||           || || 162.88      ||           ||
|| _64-bit values_        || || 958.20           || 518.56        ||         || || 1,139.69  ||                 ||           ||           || || 150.68†     ||           ||
|| _128-bit values_       || || 361.50           || 293.92        ||         || || 436.78    ||                 ||           ||           || || 81.59††     ||           ||


  ====<FONT color=#006666>32-bit `uint` Keys (10^6^ elements/s)</FONT>====

|| *Satellite Values*     || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _None (keys-only)_     || || 741.52           || 580.41        ||         || || 1,005.49  ||                 || 616.73    || 535.65    || || 265.19      ||           ||
|| _32-bit values_        || || 581.14           || 350.01        ||         || || 775.12    ||                 || 489.85    || 449.80    || || 188.71†     ||           ||
|| _64-bit values_        || || 423.85           || 221.88        ||         || || 492.28    ||                 ||           ||           || || 169.73†     ||           ||
|| _128-bit values_       || || 172.65           || 134.56        ||         || || 206.16    ||                 ||           ||           || || 54.86††     ||           ||

  
  ====<FONT color=#006666>64-bit `ulong` Keys (10^6^ elements/s)</FONT>====

|| *Input Data*           || || *C2050 (No ECC)* || *C2050 (ECC)* || *C1060* || || *GTX 480* || *GTX 460 (1GB)* || *GTX 285* || *GTX 280* || || *9800 GTX+* || *8800 GTX ||
|| _None (keys-only)_     || || 240.3            || 161.1         ||         || || 304.86    ||                 ||           ||           || || 70.21†      ||           ||
|| _32-bit values_        || || 190.9            || 105.8         ||         || || 224.30    ||                 ||           ||           || || 54.86†      ||           ||
|| _64-bit values_        || || 157.6            || 91.01         ||         || || 186.40    ||                 ||           ||           || || 51.20††     ||           ||
|| _128-bit values_       || || 76.08            || 59.41         ||         || || 90.61†    ||                 ||           ||           || || 24.23††     ||           ||

† 16M+ elements (restricted by global memory size)<BR>
†† 8M+ elements (restricted by global memory size)



<BR><BR>
----

=Building and Adapting=

The current version is: *1.0.1 (SVN revision 137)*

Like [http://code.google.com/p/thrust/ Thrust], the sources for our implementation are designed to be included and built as part of larger applications (as opposed to linked in as a shared or static library).  This prevents kernel bloat and allows the implementation to tailor itself to your data types. The current implementation can sort:
 * Keys of any C/C++ built-in, numeric type (e.g., `signed char`, `float`, `unsigned long long`, etc.)
 * Any structured payload type (within reason).

You can obtain the source code for SRTS Radix Sort (and other B40C subprojects from [http://code.google.com/p/back40computing/source/checkout the SVN repository].  This subproject can be found in the `/trunk/SrtsRadixSort` subtree.  It contains C++/CUDA code for:

 * The GPU kernel sources
 * A templated API for invoking a sorting operation from a host program.  By including the sorting API like a header file, `nvcc` will specialize kernels and runtime code needed to sort the data types specified by the enclosing application.
 * A simple toy program (and a more advanced benchtest program) that demonstrate how the sorting implementation can be integrated into and called from host programs.

The subproject sources require compilation using CUDA Toolkit version 3.0 or higher.  The repository includes a `Makefile` to build the example programs on Linux.  This Makefile can be used as a guide for constructing an equivalent VC++ project for evaluation on Windows systems.  In addition, we suggest that you compile the kernels with the following options:

 * 32-bit device pointers, regardless of whether we're on a 64-bit machine or not.  64-bit device pointers incur ~10-15% slowdown because it causes fairly increased kernel register counts which prevents us from being able to meet our targeted occupancies.
 * Without the newly-introduced-for-3.1 ABI (that gives programmers a program stack).  Even though this code does not utilize a program stack for sorting, the ABI incurs extra register pressure regardless and results in a ~1-5% slowdown.   



<BR><BR>
----

Cheers!